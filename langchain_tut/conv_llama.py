import os
import sys
from datetime import datetime as dt
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain_core.messages import SystemMessage
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain_groq import ChatGroq
from langchain_core.tools import tool, render_text_description
from langchain_core.output_parsers import StrOutputParser
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms.ollama import Ollama
from langchain_community.tools import DuckDuckGoSearchRun


# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_API_KEY"] = str(os.getenv("LANGCHAIN_API_KEY"))
# os.environ["LANGCHAIN_PROJECT"] = "pr-bumpy-succotash-85"
# os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"


@tool("multiply_tool", return_direct=True, response_format="content_and_artifact")
def multiply(a: int, b: int) -> int:
    """Multiply two integers.

    Args:
        a: First integer
        b: Second integer
    """
    return a * b


@tool("add_tool", return_direct=True, response_format="content_and_artifact")
def add(a: int, b: int) -> int:
    """Add two integers.

    Args:
        a: First integer
        b: Second integer
    """
    return a + b

    


def main():
    """
    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.
    """

    # Get Groq API key
    groq_api_key = os.environ["GROQ_API_KEY"]
    model = 'llama3.1'

    llm = Ollama(model=model)

    tools = [add, multiply]
    rendered_tools = render_text_description(tools)

    print(" ")

    system_prompt = f"""
        Your name is Jarvis, you are a friendly conversational assistant.
        You're funny and always give short answers.
        you dont think that your an AI.
        You are an assistant that has access to the following set of tools. 
        Here are the names and descriptions for each tool:

        {rendered_tools}
        
    """

    # Construct a chat prompt template using various components
    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content=system_prompt
            ),  # This is the persistent system prompt that is always included at the start of the chat.
            MessagesPlaceholder(
                variable_name="chat_history",
                optional=True
            ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.
            HumanMessagePromptTemplate.from_template(
                "{human_input}"
            ),  # This template is where the user's current input will be injected into the prompt.
        ]
    )
    output_parser = StrOutputParser()
    # Create a conversation chain using the LangChain LLM (Language Learning Model)
    conversation = prompt | llm | output_parser 
    chat_history = [
        # {'role': 'human', 'content': "My name is Darwin"},
        # {'role': 'human', 'content': "I lived in San pedro laguna"},
        # {'role': 'human', 'content': f"Current date today {dt.today()}"},
    ]

    # The chatbot's answer is generated by sending the full prompt to the Groq API.
    response = conversation.invoke({
        "human_input": "Greet me, my name is Darwin",
        "chat_history": chat_history
    })
    print("Jarvis:", response)
    print(" ")

    while True:
        user_question = input("Ask a question: ")

        if user_question == 'exit':
            sys.exit(0)

        # The chatbot's answer is generated by sending the full prompt to the Groq API.
        response = conversation.invoke({
            "human_input": user_question,
            "chat_history": chat_history
        })
        print("Jarvis:", response)
        print(" ")

        chat_history.append(user_question)

        if len(chat_history) > 5:
            del chat_history[:-5]

if __name__ == "__main__":
    main()
